{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from typing import Tuple,Union,List\n",
    "from os.path import splitext\n",
    "from itertools import chain\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['xtick.major.pad']='8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load(\"de_core_news_sm\",disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Options for spacy nlp.pipe:\n",
    "N_CORES = 3 \n",
    "BATCH_SIZE = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_file(df : Union[pd.DataFrame,pd.Series], filepath : str):\n",
    "    '''\n",
    "    Saves dataframe to file in utf-8, or pickles it. File extension \n",
    "    can be either 'csv' or 'pkl'.'''\n",
    "    _, ext = splitext(filepath)\n",
    "    if ext == \".csv\":\n",
    "        df.to_csv(\n",
    "            filepath.encode('unicode-escape').decode(),\n",
    "            header=True,encoding=\"utf-8\"\n",
    "            )\n",
    "    elif ext == \".pkl\":\n",
    "        df.to_pickle(\n",
    "            filepath.encode('unicode-escape').decode()\n",
    "            )\n",
    "    else:\n",
    "        raise Exception(\"File extention must be either 'csv' or 'pkl'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_FNDatasetGer() -> pd.DataFrame:\n",
    "    FNDatasetGerman = pd.read_csv(r\"datasets/FNDatasetGer.csv\",encoding=\"utf-8\")\n",
    "    FNDatasetGerman.drop(columns=[\"id\",\"url\",\"Kategorie\",\"Datum\",\"Quelle\",\"Art\"],inplace=True)\n",
    "    FNDatasetGerman = FNDatasetGerman.convert_dtypes().astype({\"Fake\":bool})\n",
    "    FNDatasetGerman.rename(columns={\"Titel\":\"Title\"},inplace=True)\n",
    "    return FNDatasetGerman\n",
    "\n",
    "def prepare_germanFakeNC() -> pd.DataFrame:\n",
    "    germanFakeNC = pd.read_json(r\"datasets/texts_GermanFakeNC.json\",encoding=\"utf-8\")\n",
    "    germanFakeNC.rename(columns={\"title\":\"Title\",\"text\":\"Body\"},inplace=True)\n",
    "    germanFakeNC[\"Fake\"] = True\n",
    "    return germanFakeNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data() -> pd.DataFrame:\n",
    "    FNDatasetGerman = prepare_FNDatasetGer()\n",
    "    germanFakeNC = prepare_germanFakeNC()\n",
    "    #integrate germanFakeNC into FNDatasetGerman:\n",
    "    combined_data = pd.concat([FNDatasetGerman,germanFakeNC])\n",
    "    combined_data.drop_duplicates(subset=\"Body\",inplace=True)\n",
    "    #save for prediction later-on:\n",
    "    save_to_file(combined_data,\"datasets/combined.pkl\")\n",
    "    return combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate(data : pd.DataFrame) -> Tuple[pd.DataFrame,pd.DataFrame]:\n",
    "    '''\n",
    "    Seperates the combined dataset into Real News and Fake News and drops the \"Fake\" column'''\n",
    "    grouped_data = data.groupby(data.Fake)\n",
    "    fn_data = grouped_data.get_group(True)\n",
    "    fn_data.name = \"FakeNews\"\n",
    "    rn_data = grouped_data.get_group(False)\n",
    "    rn_data.name = \"RealNews\"\n",
    "    fn_data.drop(\"Fake\",inplace=True,axis=1)\n",
    "    rn_data.drop(\"Fake\",inplace=True,axis=1)\n",
    "    return rn_data,fn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(texts : pd.Series) -> pd.DataFrame:\n",
    "    '''\n",
    "    Takes Series of texts and analyses on the word-token level.\n",
    "    Returns DataFrame with columns: \n",
    "    - Token, \\n\n",
    "    - Lemma, \\n\n",
    "    - POS (universal dependencies schema), \\n\n",
    "    - Tag (finegrained tag schema), \\n\n",
    "    - is_punctuation (whether the Token consists of pure punctuation characters) \\n\n",
    "    - is_spacy (whether the Token consists of white-space characters, e.g. \"\\\\r\\\\n\"\\n\n",
    "    Each token has one row dedicated to it.'''\n",
    "    return pd.DataFrame(\n",
    "            list((i,\n",
    "                token.text if not token.is_punct else token.text[:3],\n",
    "                token.lemma_,\n",
    "                token.pos_,\n",
    "                token.tag_,\n",
    "                token.is_stop,\n",
    "                token.is_punct,\n",
    "                token.is_space) \n",
    "                for i,doc in \n",
    "                enumerate(nlp.pipe(texts,batch_size=BATCH_SIZE,n_process=N_CORES)) \n",
    "                for token in doc\n",
    "            ),\n",
    "            columns=[\"i\",\"Token\",\"Lemma\",\"POS\",\"Tag\",\"is_stopword\",\"is_punctuation\",\"is_space\"]\n",
    "        ).set_index(\"i\")#DON'T SET inplace=True: returns None?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_grams(texts : pd.Series) -> List[pd.DataFrame]:\n",
    "    '''\n",
    "    Take series of texts and returns bigrams and trigrams for tokens and lemmas each. \n",
    "    Instead of a data frame, the columns/series are returned in a list. \n",
    "    Each n-gram has it's own row'''\n",
    "    def generate_ngrams(doc, n :int):\n",
    "        # Helper function. Creates the N-grams like this: \n",
    "        # [Doc1:\n",
    "        # [(\"tok1 - tok2\"),(\"lem1 - lem2\"),\n",
    "        #  (\"tok2 - tok3\"),(\"lem2 - lem3\"),\n",
    "        #  (\"tok3 - tok4\"),(\"lem3 - lem4\")],\n",
    "        #  Doc2:\n",
    "        # [(\"tok1 - tok2\"),... and so on\n",
    "        for ngram in list(zip(*[doc[i:] for i in range(n)])):\n",
    "            if not any(token.is_space or token.is_stop or token.is_punct for token in ngram):\n",
    "                yield (\" \".join(token.text   for token in ngram),\n",
    "                       \" \".join(token.lemma_ for token in ngram))\n",
    "        ########\n",
    "    #gram gets the n-grams for each document:\n",
    "    #(tagger still enabled for accurate lemmatization)\n",
    "    gram = lambda texts,n: (\n",
    "             generate_ngrams(doc,n)\n",
    "             for doc \n",
    "             in  nlp.pipe(texts,batch_size=BATCH_SIZE,n_process=N_CORES,disable=[\"parser\"])\n",
    "    )\n",
    "    bigram_df = pd.DataFrame(\n",
    "        list(chain.from_iterable(gram(texts,2))),\n",
    "        columns=[\"Token Bigrams\",\"Lemma Bigrams\"]\n",
    "    )\n",
    "    trigram_df = pd.DataFrame(\n",
    "        list(chain.from_iterable(gram(texts,3))),\n",
    "        columns=[\"Token Trigrams\",\"Lemma Trigrams\"]\n",
    "    )\n",
    "    return [bigram_df[\"Token Bigrams\"],\n",
    "            bigram_df[\"Lemma Bigrams\"],\n",
    "            trigram_df[\"Token Trigrams\"],\n",
    "            trigram_df[\"Lemma Trigrams\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentencize(texts : pd.Series) -> pd.Series:\n",
    "    '''\n",
    "    Takes a series of texts, and returns an equvalent series \n",
    "    where each row contains this row's sentences.\\n\n",
    "    Note that unlike n_grams() and tokenize(), here, the sentences don't have a row each. '''\n",
    "    # get_sents = lambda doc: [sent for sent in nlp(doc,disable=[\"tagger\"]).sents]\n",
    "    # return series.apply(get_sents).rename(\"Sent_Tokenized_Doc\",inplace=True)\n",
    "    return pd.Series(\n",
    "        list(list(doc.sents) \n",
    "             for doc \n",
    "             in nlp.pipe(texts,batch_size=BATCH_SIZE,n_process=N_CORES,disable=[\"tagger\"])\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ColumnStats(col : pd.Series, new_name:str=None) -> pd.Series:\n",
    "    '''\n",
    "    Take a data frame columns / series and returns the most important statistical measures \n",
    "    for the length of each row (like mean, std, median). Optionally renames the series:\\n\n",
    "    new_name : str or None'''\n",
    "    if not new_name:\n",
    "    # str.len can be used for anything where the len function is defined:\n",
    "        return col.str.len().describe()\n",
    "    return col.str.len().describe().rename(new_name,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_freqdists(series_list:List[pd.Series],n:int,normalize:bool)->List[pd.Series]:\n",
    "    '''\n",
    "    Takes a list of series and returns the sorted counts of the n most common values.'''\n",
    "    return [series.value_counts(normalize=normalize)[:n].copy() for series in series_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokendata_freqdists(df :pd.DataFrame,n:int, normalize:bool=True)-> List[pd.Series]:\n",
    "    '''\n",
    "    Takes a tokenized data frame and returns the frequencies of \n",
    "    word-tokens, lemmas, POS-tags, finegrained tags and punctuation-tokens. \n",
    "    The frequency distributions are returned as a list of series'.'''\n",
    "    not_PunctStopSpace   =  df.groupby([\"is_stopword\",\"is_punctuation\",\"is_space\"],\n",
    "                                        sort = False).get_group((False,False,False))\n",
    "    punct_space_grouping =  df.groupby([\"is_punctuation\",\"is_space\"],sort=False)\n",
    "    notPuncts  =            punct_space_grouping.get_group((False,False))\n",
    "    puncts     =            punct_space_grouping.get_group((True,False))\n",
    "\n",
    "    return create_freqdists([\n",
    "        not_PunctStopSpace[\"Token\"].rename(\"Token Frequencies\"),\n",
    "        not_PunctStopSpace[\"Lemma\"].rename(\"Lemma Frequencies\"),\n",
    "        notPuncts[\"POS\"]           .rename(\"POS-Tag Frequencies\"),\n",
    "        notPuncts[\"Tag\"]           .rename(\"Finegrained Tag Frequencies\"),\n",
    "        puncts[\"Token\"]            .rename(\"Punct-Token Frequencies\"),\n",
    "    ],n=n,normalize=normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_analysis(df_col :pd.Series,df_name:str,n:int,normalize:bool):\n",
    "    '''\n",
    "    Takes a data frame columns / series of texts and analyzes each row's tokens. \n",
    "    The frequency distributions are saved as plots, but also in .csv format. \n",
    "    Tokenlengths are also analyzed (mean len, median len and std) and are saved as .csv.\n",
    "    '''\n",
    "    tokendata = tokenize(df_col)\n",
    "\n",
    "    #avg number of tokens per article:\n",
    "    tokens_per_text_stats = get_ColumnStats(\n",
    "            pd.Series(tokendata[\"Token\"].groupby(tokendata.index,sort=False).groups),\n",
    "            new_name=\"Tokens per Text\"\n",
    "        )\n",
    "    save_to_file(tokens_per_text_stats,\"stats/%s_%s_Tokens_per_Text.csv\"%(df_name,df_col.name))\n",
    "    \n",
    "    #Calculate distributions of tokens, lemmas, POS-tags, fine tags, punctuation:\n",
    "    for freqdist in get_tokendata_freqdists(tokendata,n=n,normalize=normalize): \n",
    "        unique_name=(df_name,df_col.name,freqdist.name)\n",
    "        save_plot(\n",
    "            freqdist,\n",
    "            title   = \"%s %s %s\"    %unique_name,\n",
    "            filepath= \"%s_%s_%s.png\"%unique_name\n",
    "        )\n",
    "        save_to_file(freqdist,\"freqdists/%s_%s_%s.csv\"%unique_name)\n",
    "    \n",
    "    #Calculate number of letters per token:\n",
    "    tokenlen_stats = get_ColumnStats(tokendata[\"Token\"],new_name=\"Letters Per Token\")\n",
    "    save_to_file(tokenlen_stats,\"stats/%s_%s_Tokenlength.csv\"%(df_name,df_col.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram_analysis(df_col :pd.Series,df_name:str,n:int,normalize:bool):\n",
    "    '''\n",
    "    Takes a data frame columns / series of texts and analyzes each row's bigrams and trigrams. \n",
    "    The results (all frequency distributions) are saved as plots, but also in .csv format.'''\n",
    "    ngram_freqdists = create_freqdists(n_grams(df_col),n=n,normalize=normalize)\n",
    "    for freqdist in ngram_freqdists:\n",
    "        unique_name=(df_name,df_col.name,freqdist.name)\n",
    "        save_plot(\n",
    "            freqdist,\n",
    "            title   = \"%s %s %s\"    %unique_name,\n",
    "            filepath= \"%s_%s_%s.png\"%unique_name\n",
    "        )\n",
    "        save_to_file(freqdist,\"freqdists/%s_%s_%s.csv\"%unique_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_analysis(df_col :pd.Series,df_name:str):\n",
    "    '''\n",
    "    Takes data frame column / series of texts and analyses the average number of \n",
    "    tokens per sentence, and the average number of sentences per article/title. \n",
    "    Results (mean, median, std) are saved in .csv format.'''\n",
    "    unique_name=(df_name,df_col.name)\n",
    "\n",
    "    sentdata = sentencize(df_col)\n",
    "    #Calculate number of sentences per article/title:\n",
    "    # sents_per_article_stats = sentdata.groupby(sentdata.index)\\\n",
    "    #     .apply(lambda x: x.to_list()).str.len().describe()\n",
    "    sents_per_article_stats = sentdata.str.len().describe()\n",
    "    sents_per_article_stats.rename(\"Sents_per_Article\",inplace=True)\n",
    "    save_to_file(sents_per_article_stats,\"stats/%s_%s_Sents_per_Article.csv\"%unique_name)\n",
    "    #Calculate number of tokens per sentence:\n",
    "    sentlen_stats = get_ColumnStats(sentdata.explode(),new_name=\"Tokens_per_Sentence\")\n",
    "    save_to_file(sentlen_stats,\"stats/%s_%s_Tokens_per_Sentence.csv\"%unique_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(df_col :pd.Series, title:str, filepath:str):\n",
    "    '''\n",
    "    Creates a plot for a frequency distribution and saves it.'''\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    x_axis = df_col.index.to_list()\n",
    "    y_axis = df_col.to_list()\n",
    "    plt.plot(x_axis,y_axis)\n",
    "    plt.title(title)\n",
    "    rotation = 90\n",
    "    if \"Punct\" in df_col.name:\n",
    "        rotation = 0\n",
    "    elif not \"Token\" in df_col.name and not \"Lemma\" in df_col.name:\n",
    "        rotation = 80\n",
    "    plt.xticks(x_axis,rotation=rotation)\n",
    "    plt.tick_params(axis='x', which='major', labelsize=7.3, pad=4.1)\n",
    "    plt.tight_layout()\n",
    "    plt.margins(0.03)\n",
    "    plt.savefig(r\"plots/\"+filepath,dpi=350,bbox_inches = \"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Program following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2040</th>\n",
       "      <td>Erdogan bleibt hart und treibt Offensive trotz...</td>\n",
       "      <td>Die Türkei will die Offensive gegen die Kurden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2041</th>\n",
       "      <td>Frankreich und Deutschland wollen EU neue Impu...</td>\n",
       "      <td>Die Parlamente von Deutschland und Frankreich ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2042</th>\n",
       "      <td>Puigdemont als Kataloniens Regierungschef zur ...</td>\n",
       "      <td>Die katalanischen Separatisten wollen den ins ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2043</th>\n",
       "      <td>UBS lockt Aktionäre mit höheren Dividenden und...</td>\n",
       "      <td>Die UBS will ihre Aktionäre angesichts trübere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2044</th>\n",
       "      <td>Ein Deutscher unter Toten nach Anschlag auf Ho...</td>\n",
       "      <td>Bei dem Anschlag auf das Hotel Intercontinenta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63790</th>\n",
       "      <td>Chrissy Teigen hat Angst vor Wochenbettdepression</td>\n",
       "      <td>Das schwangere Model fühlt sich diesmal aber f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63863</th>\n",
       "      <td>Lehrer entging durch Hochzeit mit Schülerin Ve...</td>\n",
       "      <td>55-Jähriger muss nach Sex mit damals 15-Jährig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63864</th>\n",
       "      <td>Warum die Taiwaner Toilettenpapier bunkern</td>\n",
       "      <td>Aus Angst vor Preiserhöhungen bei Klopapier ka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63866</th>\n",
       "      <td>\\r\\nDie neue Premium-Klasse von Samsung\\r\\n   ...</td>\n",
       "      <td>Am Vorabend der Eröffnung des Mobile World Con...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63867</th>\n",
       "      <td>Dirigent Gustav Kuhn wehrt sich gegen \"haltlos...</td>\n",
       "      <td>Gustav Kuhn, künstlerischer Leiter der Festspi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>57953 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Title  \\\n",
       "2040   Erdogan bleibt hart und treibt Offensive trotz...   \n",
       "2041   Frankreich und Deutschland wollen EU neue Impu...   \n",
       "2042   Puigdemont als Kataloniens Regierungschef zur ...   \n",
       "2043   UBS lockt Aktionäre mit höheren Dividenden und...   \n",
       "2044   Ein Deutscher unter Toten nach Anschlag auf Ho...   \n",
       "...                                                  ...   \n",
       "63790  Chrissy Teigen hat Angst vor Wochenbettdepression   \n",
       "63863  Lehrer entging durch Hochzeit mit Schülerin Ve...   \n",
       "63864         Warum die Taiwaner Toilettenpapier bunkern   \n",
       "63866  \\r\\nDie neue Premium-Klasse von Samsung\\r\\n   ...   \n",
       "63867  Dirigent Gustav Kuhn wehrt sich gegen \"haltlos...   \n",
       "\n",
       "                                                    Body  \n",
       "2040   Die Türkei will die Offensive gegen die Kurden...  \n",
       "2041   Die Parlamente von Deutschland und Frankreich ...  \n",
       "2042   Die katalanischen Separatisten wollen den ins ...  \n",
       "2043   Die UBS will ihre Aktionäre angesichts trübere...  \n",
       "2044   Bei dem Anschlag auf das Hotel Intercontinenta...  \n",
       "...                                                  ...  \n",
       "63790  Das schwangere Model fühlt sich diesmal aber f...  \n",
       "63863  55-Jähriger muss nach Sex mit damals 15-Jährig...  \n",
       "63864  Aus Angst vor Preiserhöhungen bei Klopapier ka...  \n",
       "63866  Am Vorabend der Eröffnung des Mobile World Con...  \n",
       "63867  Gustav Kuhn, künstlerischer Leiter der Festspi...  \n",
       "\n",
       "[57953 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realN_data, fakeN_data = seperate(prepare_data())\n",
    "realN_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FakeNews'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fakeN_data.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell is basically the main method. The computational load and time needed are very large, because of the size of the data set (even with optimizations!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalize=True\n",
    "n_most_common = 50\n",
    "for data in [fakeN_data, realN_data]:\n",
    "    for colname in data:\n",
    "        token_analysis(data[colname],data.name,n=n_most_common,normalize=normalize)\n",
    "        ngram_analysis(data[colname],data.name,n=n_most_common,normalize=normalize)\n",
    "        sent_analysis (data[colname],data.name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
